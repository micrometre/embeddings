---
title: "Building a Semantic Quiz with AI-Powered Answer Grading in the Browser"
description: "Learn how to create an interactive quiz that grades free-text answers using semantic similarity - all running 100% client-side with Transformers.js"
pubDate: "December 6, 2025"
author: "Henok Wehibe"
tags: ["ai", "transformers.js", "javascript", "tutorial", "embeddings"]
featured: true
image: "/images/semantic-quiz-hero.png"
readingTime: "10 min read"
---

# Building a Semantic Quiz with AI-Powered Answer Grading in the Browser

What if your quiz could understand *meaning* instead of just matching exact text? Imagine a system where "Paris is the capital" and "The capital of France is Paris" both get full credit â€” because they mean the same thing.

In this article, I'll show you how I built an interactive quiz with **semantic answer grading** using Transformers.js â€” all running 100% in the browser with no server required. The best part? It uses simple, universally-known questions that anyone can test without specialized knowledge.

## The Problem with Traditional Quizzes

Traditional quiz systems are frustrating:

- âŒ **Exact match only** â€” "gradient descent" â‰  "Gradient Descent"
- âŒ **No paraphrasing** â€” Your own words don't count
- âŒ **Regex nightmares** â€” Maintaining answer patterns is painful
- âŒ **Limited feedback** â€” Just "right" or "wrong"

What if we could grade answers based on their *semantic meaning* instead?

## Enter Transformers.js

[Transformers.js](https://huggingface.co/docs/transformers.js) is Hugging Face's library for running transformer models directly in the browser using WebAssembly (WASM). It brings the power of models like BERT, GPT-2, and sentence transformers to the client-side.

### Why Browser-Based?

| Benefit | Description |
|---------|-------------|
| ğŸ” **Privacy** | Answers never leave the user's device |
| ğŸ’° **Zero cost** | No API calls or server infrastructure |
| âš¡ **Offline** | Works without internet after initial load |
| ğŸš€ **Low latency** | No network round-trips |

## How Semantic Grading Works

The magic happens in three steps:

### 1. Text â†’ Embeddings

We convert text into **embedding vectors** â€” lists of numbers that capture meaning. The model we use (`all-MiniLM-L6-v2`) produces 384-dimensional vectors.

```javascript
import { pipeline } from '@huggingface/transformers';

// Load the embedding model
const embedder = await pipeline(
  'feature-extraction', 
  'Xenova/all-MiniLM-L6-v2',
  { dtype: 'q8' } // Quantized for faster loading
);

// Generate embedding for text
async function embed(text) {
  const output = await embedder(text, { 
    pooling: 'mean', 
    normalize: true 
  });
  return Array.from(output.data);
}
```

### 2. Compare with Cosine Similarity

To compare two answers, we calculate the **cosine similarity** between their vectors. This measures the angle between them â€” smaller angle means more similar meaning.

```javascript
function cosineSimilarity(a, b) {
  let dotProduct = 0;
  let normA = 0;
  let normB = 0;
  
  for (let i = 0; i < a.length; i++) {
    dotProduct += a[i] * b[i];
    normA += a[i] * a[i];
    normB += b[i] * b[i];
  }
  
  return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));
}
```

The formula:

$$
\text{similarity} = \frac{A \cdot B}{\|A\| \times \|B\|}
$$

Where:
- $A \cdot B$ is the dot product of the vectors
- $\|A\|$ and $\|B\|$ are the magnitudes (lengths) of each vector

### 3. Grade Based on Similarity Score

We map similarity scores to grades:

| Similarity | Rating | Points | Meaning |
|------------|--------|--------|---------|
| â‰¥ 85% | Excellent! | 1.0 | Core concept captured accurately |
| â‰¥ 70% | Good! | 0.8 | Good understanding with minor gaps |
| â‰¥ 55% | Partial | 0.5 | Some relevant concepts mentioned |
| < 55% | Try Again | 0 | Answer doesn't match expected concept |

## Building the Quiz

Let's build it step by step.

### Project Setup

```bash
npm create vite@latest browser-quiz
cd browser-quiz
npm install @huggingface/transformers
```

### Quiz Data Structure

Define questions with expected answers and hints. I chose simple, universally-known questions so anyone can test the semantic matching:

```javascript
const quizData = [
  {
    question: "What is the capital city of France?",
    answer: "Paris is the capital city of France, located in the northern part of the country along the Seine River.",
    hint: "It's famous for the Eiffel Tower and is known as the City of Light."
  },
  {
    question: "What do plants need to make their own food?",
    answer: "Plants need sunlight, water, and carbon dioxide to perform photosynthesis and produce glucose for energy.",
    hint: "Think about what plants get from the sun, soil, and air."
  },
  {
    question: "Why is the sky blue?",
    answer: "The sky appears blue because sunlight is scattered by the atmosphere, and blue light is scattered more than other colors due to its shorter wavelength.",
    hint: "It has to do with how light interacts with air molecules."
  },
  // ... more questions
];
```

### Pre-compute Answer Embeddings

For efficiency, we compute answer embeddings once at startup:

```javascript
let answerEmbeddings = [];

async function initializeQuizEmbeddings() {
  console.log('Pre-computing quiz answer embeddings...');
  
  for (const quiz of quizData) {
    const embedding = await embed(quiz.answer);
    answerEmbeddings.push(embedding);
  }
  
  console.log('Quiz embeddings ready!');
}
```

### Evaluate User Answers

When a user submits an answer, we compare it to the expected answer:

```javascript
async function evaluateAnswer(userAnswer, questionIndex) {
  // Get embedding for user's answer
  const userEmbedding = await embed(userAnswer);
  
  // Compare with expected answer
  const expectedEmbedding = answerEmbeddings[questionIndex];
  const similarity = cosineSimilarity(userEmbedding, expectedEmbedding);
  
  // Determine grade
  if (similarity >= 0.85) {
    return { label: 'Excellent!', points: 1.0, similarity };
  } else if (similarity >= 0.70) {
    return { label: 'Good!', points: 0.8, similarity };
  } else if (similarity >= 0.55) {
    return { label: 'Partial', points: 0.5, similarity };
  } else {
    return { label: 'Try Again', points: 0, similarity };
  }
}
```

## The Complete UI

Here's how I structured the quiz interface:

```html
<div class="quiz-section">
  <h2>ğŸ¯ Interactive ML Quiz</h2>
  
  <div class="quiz-header">
    <span id="quiz-progress">Question 1 of 5</span>
    <span id="quiz-score">Score: 0/0 (0%)</span>
  </div>
  
  <div class="quiz-question" id="quiz-question">
    Loading quiz...
  </div>
  
  <textarea 
    id="quiz-answer" 
    placeholder="Type your answer here..."
    disabled
  ></textarea>
  
  <div class="quiz-buttons">
    <button id="submit-answer-btn">Submit Answer</button>
    <button id="show-hint-btn">Show Hint</button>
    <button id="next-question-btn" style="display: none;">
      Next Question
    </button>
  </div>
  
  <div id="quiz-feedback"></div>
</div>
```

## Real-World Examples

Here's how semantic matching performs on actual answers. I chose simple questions so you can easily verify the results:

### Question: "What is the capital city of France?"

| User Answer | Similarity | Grade |
|-------------|------------|-------|
| "Paris is the capital of France" | 92% | âœ… Excellent |
| "The capital is Paris" | 85% | âœ… Excellent |
| "Paris" | 78% | âœ… Good |
| "A city in Europe" | 48% | âŒ Try Again |

### Question: "Why is the sky blue?"

| User Answer | Similarity | Grade |
|-------------|------------|-------|
| "Light scattering in the atmosphere makes blue visible" | 88% | âœ… Excellent |
| "Blue light scatters more than other colors" | 82% | âœ… Good |
| "Because of the sun" | 55% | âš ï¸ Partial |
| "It just is" | 32% | âŒ Try Again |

The model understands that different phrasings of the same concept should receive credit!

## Performance Considerations

### Model Loading

The `all-MiniLM-L6-v2` model is ~25MB (quantized). First load downloads it, but subsequent loads use the browser cache:

```javascript
const embedder = await pipeline('feature-extraction', 'Xenova/all-MiniLM-L6-v2', {
  dtype: 'q8', // 8-bit quantization for smaller size
  progress_callback: (progress) => {
    console.log(`Loading: ${Math.round(progress.progress)}%`);
  }
});
```

### Embedding Speed

On my machine, generating an embedding takes ~50-100ms per sentence. For a quiz with 5-10 questions, pre-computing embeddings adds 0.5-1 second to startup.

### Memory Usage

The model uses ~100-150MB of memory. This is reasonable for modern devices but worth considering for mobile users.

## Bonus: Vector Search for Related Questions

I also built a vector index for finding semantically similar questions:

```javascript
import { VectorIndex } from './vectorIndex.js';

// Create index with 384 dimensions
const vectorIndex = new VectorIndex(384);

// Index includes questions like:
const questions = [
  "What is the capital city of France?",
  "How many continents are there on Earth?",
  "What planet is known as the Red Planet?",
  "What is the largest ocean on Earth?",
  // ... more questions
];

// Add question embeddings
for (const question of questions) {
  const embedding = await embed(question);
  vectorIndex.add(embedding, { text: question });
}

// Search for similar questions
const results = vectorIndex.search(queryEmbedding, 3);
// Returns top 3 most similar questions
```

This powers a search feature where users can find related topics! Try searching:
- "French capital" â†’ finds "What is the capital city of France?"
- "Mars" â†’ finds "What planet is known as the Red Planet?"
- "ocean size" â†’ finds "What is the largest ocean on Earth?"

## Try It Yourself

I've open-sourced the complete project:

ğŸ‘‰ **[Browser Embeddings Quiz on GitHub](https://github.com/micrometre/browser-embeddings)**

Clone it and run:

```bash
git clone https://github.com/micrometre/browser-embeddings
cd browser-embeddings
npm install
npm run dev
```

## What's Next?

This approach opens up exciting possibilities:

- ğŸ“ **Essay grading** â€” Grade long-form answers semantically
- ğŸŒ **Multi-language** â€” Same embeddings work across languages
- ğŸ“ **Adaptive learning** â€” Suggest topics based on weak answers
- ğŸ’¬ **Conversational quizzes** â€” Natural language Q&A

## Conclusion

By combining Transformers.js with cosine similarity, we've built a quiz system that:

- âœ… Understands meaning, not just exact text
- âœ… Runs 100% in the browser
- âœ… Respects user privacy
- âœ… Works offline after initial load
- âœ… Provides nuanced feedback

The future of educational technology isn't just about grading right or wrong â€” it's about understanding *how* someone answered and providing meaningful feedback.

---

Have questions or built something cool with Transformers.js? I'd love to hear about it!

[Get In Touch](https://henok.cloud/contact)
